import asyncio
import os
import queue
from contextlib import asynccontextmanager
from typing import Set

import socketio
from engineio.payload import Payload
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles

from audio.audiobroadcaster import AudioBroadcaster
from audio.audiostreamer import WebAudioStreamer
from audio.transcriptionconsumer import TranscriptionConsumer
from common.arguments import arguments
from common.logger import logger
from db import *  # noqa: F401,F403
from db import engine  # Explicit import for type checker
from hardware.soapysdrbrowser import discover_soapy_servers
from processing.processmanager import process_manager
from server import shutdown
from server.firsttime import first_time_initialization, run_initial_sync
from server.scheduler import start_scheduler, stop_scheduler
from server.version import get_full_version_info
from tracker.messages import handle_tracker_messages
from tracker.runner import start_tracker_process

# Increase payload limits to handle large waterfall PNG images (typically 2-5MB as base64)
Payload.max_decode_packets = 50
# Default is 100KB (100000 bytes), increase to 10MB to handle large PNG images
Payload.max_decode_packet_size = 10 * 1024 * 1024  # 10MB

# At the top of the file, add a global to track background tasks
background_tasks: Set[asyncio.Task] = set()

# Module-level variable to track if initial sync is needed
_needs_initial_sync: bool = False

# Audio distribution system
# Demodulators write to audio_queue, AudioBroadcaster distributes to multiple consumers
# Keep input queue small (5-10 chunks) to minimize tuning lag
# At 1024 samples/chunk @ 44.1kHz = 23ms/chunk, 10 chunks = 230ms latency
audio_queue: queue.Queue = queue.Queue(maxsize=10)
audio_broadcaster: AudioBroadcaster = AudioBroadcaster(audio_queue)


async def run_discover_soapy():
    """Background task to periodically discover SoapySDR servers."""
    while True:
        await discover_soapy_servers()
        await asyncio.sleep(120)


@asynccontextmanager
async def lifespan(fastapiapp: FastAPI):
    """Custom lifespan for FastAPI."""
    logger.info("FastAPI lifespan startup...")
    start_tracker_process()
    event_loop = asyncio.get_event_loop()

    # Start audio broadcaster
    audio_broadcaster.start()
    shutdown.audio_broadcaster = audio_broadcaster

    # Subscribe consumers to broadcaster
    playback_queue = audio_broadcaster.subscribe("playback", maxsize=10)
    shutdown.audio_consumer = WebAudioStreamer(playback_queue, sio, event_loop)
    shutdown.audio_consumer.start()

    # Start transcription consumer (will be used when DeBabel URL is configured)
    transcription_queue = audio_broadcaster.subscribe("transcription", maxsize=50)
    # DeBabel URL will be fetched from preferences on first use
    shutdown.transcription_consumer = TranscriptionConsumer(
        transcription_queue, sio, event_loop, debabel_url=""
    )
    shutdown.transcription_consumer.start()
    logger.info("Transcription consumer started (will connect when DeBabel URL is configured)")

    asyncio.create_task(handle_tracker_messages(sio))
    if arguments.runonce_soapy_discovery:
        await discover_soapy_servers()
    if arguments.enable_soapy_discovery:
        asyncio.create_task(run_discover_soapy())

    # Schedule initial sync if needed
    if _needs_initial_sync:
        asyncio.create_task(run_initial_sync(sio))

    # Start the background task scheduler
    start_scheduler(sio)

    # Start performance monitoring
    process_manager.start_monitoring()
    logger.info("Performance monitoring started")

    try:
        yield
    finally:
        logger.info("FastAPI lifespan cleanup...")
        stop_scheduler()
        process_manager.shutdown()
        shutdown.cleanup_everything()


sio = socketio.AsyncServer(
    async_mode="asgi",
    cors_allowed_origins="*",
    logger=True,
    engineio_logger=True,
    binary=True,
    max_http_buffer_size=10 * 1024 * 1024,  # 10MB to handle large waterfall PNG images
)
app = FastAPI(
    lifespan=lifespan,
    title="Ground Station API",
    description="API for satellite tracking, SDR control, and radio communication",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc",
    openapi_url="/api/openapi.json",
)
socket_app = socketio.ASGIApp(sio, other_asgi_app=app)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

process_manager.set_sio(sio)

# Mount static directories
app.mount("/satimages", StaticFiles(directory="satimages"), name="satimages")

# Mount data directories for recordings, snapshots, and decoded data (SSTV, AFSK, Morse, etc.)
# Ensure these directories exist before mounting
backend_dir = os.path.dirname(os.path.abspath(__file__))
recordings_dir = os.path.join(backend_dir, "..", "data", "recordings")
snapshots_dir = os.path.join(backend_dir, "..", "data", "snapshots")
decoded_dir = os.path.join(backend_dir, "..", "data", "decoded")

# Create directories if they don't exist
os.makedirs(recordings_dir, exist_ok=True)
os.makedirs(snapshots_dir, exist_ok=True)
os.makedirs(decoded_dir, exist_ok=True)

# Use html=True to enable directory browsing
app.mount("/recordings", StaticFiles(directory=recordings_dir, html=True), name="recordings")
app.mount("/snapshots", StaticFiles(directory=snapshots_dir, html=True), name="snapshots")
app.mount("/decoded", StaticFiles(directory=decoded_dir, html=True), name="decoded")


# Add the version API endpoint BEFORE the catch-all route
@app.get("/api/version")
async def get_version():
    """Return the current version information of the application."""
    try:
        logger.info("Version request received")
        version_info = get_full_version_info()
        logger.info(f"Returning version info: {version_info}")
        return version_info
    except Exception as e:
        logger.error(f"Error retrieving version information: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Failed to retrieve version information: {str(e)}"
        )


# This catch-all route comes AFTER specific API routes
@app.get("/{full_path:path}")
async def serve_spa(request: Request, full_path: str):
    static_files_dir = os.environ.get("STATIC_FILES_DIR", "../../frontend/dist")
    if full_path.startswith(("static/", "assets/", "favicon.ico")):
        return FileResponse(os.path.join(static_files_dir, full_path))
    return FileResponse(os.path.join(static_files_dir, "index.html"))


async def init_db():
    """Initialize database and run migrations."""
    global _needs_initial_sync

    logger.info("Initializing database...")

    # Ensure required data directories exist
    logger.info("Ensuring data directories exist...")
    backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_dirs = [
        os.path.join(backend_dir, "data", "db"),
        os.path.join(backend_dir, "data", "recordings"),
        os.path.join(backend_dir, "data", "snapshots"),
        os.path.join(
            backend_dir, "data", "decoded"
        ),  # For SSTV images, AFSK packets, Morse audio, etc.
        os.path.join(backend_dir, "data", "configs"),  # For satellite decoder configurations
        os.path.join(backend_dir, "data", "uhd_images"),  # For UHD FPGA images
        os.path.join(backend_dir, "data", "uhd_config"),  # For UHD configuration files
    ]
    for directory in data_dirs:
        os.makedirs(directory, exist_ok=True)
        logger.info(f"Ensured directory exists: {directory}")

    # Check if database exists by trying to query metadata
    database_existed = False
    try:
        async with engine.begin() as conn:
            # Try to get table names - if this succeeds, database exists
            result = await conn.run_sync(
                lambda sync_conn: engine.dialect.get_table_names(sync_conn)
            )
            database_existed = len(result) > 0
    except Exception:
        # Database doesn't exist or is empty
        database_existed = False

    # Run Alembic migrations to ensure schema is up to date
    logger.info("Running database migrations...")
    try:
        import concurrent.futures

        from db.migrations import run_migrations

        # Run migrations in a thread pool to avoid event loop conflicts
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            await loop.run_in_executor(executor, run_migrations)

        logger.info("Database migrations completed successfully")
    except Exception as e:
        logger.error(f"Error running database migrations: {e}")
        logger.exception(e)
        raise

    # If database didn't exist before, populate with initial data
    if not database_existed:
        logger.info("New database detected. Populating with initial data...")
        await first_time_initialization()
        _needs_initial_sync = True

    logger.info("Database initialized.")
